sum(songs$year==2010)
sum(songs$artistname=="Michael Jackson")
MJ=subset(songs, songs$artistname=="Michael Jackson")
MJ
table(MJ$songtitle)
MJ$songtitle
MJ$songtitle=factor(MJ$songtitle)
table(MJ$songtitle, MJ$Top10)
MichaelJackson$songtitle,
MichaelJackson$songtitle
MJ$songtitle
str(songs)
table(songs$timesignature)
which.max(songs$tempo)
songs[6206]
songs$songtitle[6206]
songtrain=subset(songs, songs$year <= 2009)
songtest=subset(songs, songs$year > 2009)
# To do this, we can use the following trick. First define a vector of variable names called nonvars #- these are the variables that we won't use in our model.
nonvars = c("year", "songtitle", "artistname", "songID", "artistID")
#To remove these variables from your training and testing sets, type the following commands in your R #console:
SongsTrain = SongsTrain[ , !(names(SongsTrain) %in% nonvars) ]
# To do this, we can use the following trick. First define a vector of variable names called nonvars #- these are the variables that we won't use in our model.
nonvars = c("year", "songtitle", "artistname", "songID", "artistID")
#To remove these variables from your training and testing sets, type the following commands in your R #console:
SongTrain = SongsTrain[ , !(names(SongsTrain) %in% nonvars) ]
# To do this, we can use the following trick. First define a vector of variable names called nonvars #- these are the variables that we won't use in our model.
nonvars = c("year", "songtitle", "artistname", "songID", "artistID")
#To remove these variables from your training and testing sets, type the following commands in your R #console:
songtrain = SongsTrain[ , !(names(SongsTrain) %in% nonvars) ]
# To do this, we can use the following trick. First define a vector of variable names called nonvars #- these are the variables that we won't use in our model.
nonvars = c("year", "songtitle", "artistname", "songID", "artistID")
#To remove these variables from your training and testing sets, type the following commands in your R #console:
songtrain = songtrain[ , !(names(songtrain) %in% nonvars) ]
songtest = songtest[ , !(names(songtest) %in% nonvars) ]
model1=glm(Top10 ~ ., family =  binomial )
model1=glm(Top10 ~ ., data =  songtrain ,family =  binomial )
summary(model1)
cor(songs$loudness, songs$energy)
USDA=read.csv("C:\\Users\\WALLI\\Desktop\\Di\\edx\\Analytic Edge\\USDA.csv")
HighSodium = as.numeric(USDA$Sodium > mean(USDA$Sodium, na.rm= TRUE))
str(HighSodium)
USDA$HighSodium = HighSodium
USDA$HighFat = as.numeric(USDA$TotalFat > mean(USDA$TotalFat, na.rm= TRUE))
table(USDA$HighSodium, USDA$HighFat)
tapply(USDA$VitaminC,USDA$HighSodium, summary, na.rm = TRUE)
mvt=read.csv("./edx/Analytic Edge/mvtWeek1.csv")
mvt=read.csv("mvtWeek1.csv")
setwd("C:/Users/WALLI/Desktop/Di/edx/Analytic Edge")
mvt=read.csv("mvtWeek1.csv")
setwd("C:/Users/WALLI/Desktop/Di/edx/Analytic Edge")
mvt=read.csv("C:/Users/WALLI/Desktop/Di/edx/Analytic Edge/mvtWeek1.csv")
DateConvert = as.Date(strptime(mvt$Date, "%m/%d/%y %H:%M"))
summary(DateConvert)
SongsLog2 = glm(Top10 ~ . - loudness, data=songtrain, family=binomial)
summary(SongsLog2)
SongsLog3 = glm(Top10 ~ . - утукпн, data=songtrain, family=binomial)
SongsLog3 = glm(Top10 ~ . - energy, data=songtrain, family=binomial)
suumary(SongsLog3)
summary(SongsLog3)
test_pred=predict(SongsLog3, type= "response", newdata = songtest)
table()
test_pred
tapply(test_pred > 0.5)
tapply(songtest,test_pred > 0.45)
table(songtest,test_pred > 0.45)
table(songtest$Top10,test_pred > 0.45)
(309+19)/nrow(test_pred)
nrow(test_pred)
str(test_pred)
length(test_pred)
(309+19)/length(test_pred)
314/373
309+19
19/(19+40)
309/(309+5)
39*3
39*3 *59.45
setwd("C:/Users/WALLI/Desktop/Di/edx/Analytic Edge")
parole=read.csv("parole.csv")
table(parole)
table(parole$violator)
str(parole)
parole$state = as.factor(parole$state)
parole$crime = as.factor(parole$crime)
summary(parole)
str(parole)
summary(parole$crime)
set.seed(144)
library(caTools)
split = sample.split(parole$violator, SplitRatio = 0.7)
train = subset(parole, split == TRUE)
test = subset(parole, split == FALSE)
200/600
model1=glm(vialator ~ . , data=train, family = binomial)
model1=glm(violator ~ . , data=train, family = binomial)
summary(model1)
e
exp(0)
exp(1.6119919)
summary(model1)
str(train)
table(train$crime)
summary(model1)
exp(-4.2411574 + 0.3869904 + 0.8867192 + 50*-0.0001756  )
table(train$state)
exp(-4.2411574 + 0.3869904 + 0.8867192 + 50*-0.0001756  + -0.1238867*3 + 12 * 0.0802954 + 0.6837143)
1/(1 + exp(-(-4.2411574 + 0.3869904 + 0.8867192 + 50*-0.0001756  + -0.1238867*3 + 12 * 0.0802954 + 0.6837143)))
test_predict=predict(model1, newdata = test, type = response)
test_predict=predict(model1, newdata = test, type = "response")
max(test_predict)
table(qualityTrain$PoorCare, predictTrain > 0.5)
getwd()
WHO=read.csv("C:\\Users\\WALLI\\Desktop\\Di\\edx\\Analytic Edge\\WHO.csv")
str(WHO)
summary(WHO)
which.min(WHO$Under15)
WHO$Country[86]
WHO$Country[which.min(WHO$Over60)]
WHO_Europe = subset(WHO, Region  =="Europe")
str(WHO_Europe)
table(USDA$HighSodium, USDA$HighFat)
USDA=read.csv("C:\\Users\\WALLI\\Desktop\\Di\\edx\\Analytic Edge\\USDA.csv")
HighSodium = as.numeric(USDA$Sodium > mean(USDA$Sodium, na.rm= TRUE))
str(HighSodium)
USDA$HighSodium = HighSodium
USDA$HighFat = as.numeric(USDA$TotalFat > mean(USDA$TotalFat, na.rm= TRUE))
table(USDA$HighSodium, USDA$HighFat)
tapply(USDA$VitaminC,USDA$HighSodium, summary, na.rm = TRUE)
mvt=read.csv("C:/Users/WALLI/Desktop/Di/edx/Analytic Edge/mvtWeek1.csv")
DateConvert = as.Date(strptime(mvt$Date, "%m/%d/%y %H:%M"))
summary(DateConvert)
mvt$Month = months(DateConvert)
mvt$Weekday = weekdays(DateConvert)
mvt$Date = DateConvert
sort(table(mvt$LocationDescription))
Top5=subset(mvt, mvt$LocationDescription %in% c("STREET", "PARKING LOT/GARAGE(NON.RESID.)", "ALLEY", "GAS STATION", "DRIVEWAY - RESIDENTIAL" ))
Top5$LocationDescription = factor(Top5$LocationDescription)
plot(CocaCola$Date, CocaCola$StockPrice, type = "l", col = "red")
setwd("C:/Users/WALLI/Desktop/Di/edx/Analytic Edge")
CPS=read.csv("CPSData.csv")
setwd("C:/Users/WALLI/Desktop/Di/edx/Analytic Edge")
CPS=read.csv("CPSData.csv")
table(CPS$Sex, is.na(CPS$Married))
CPS=read.csv("C:/Users/WALLI/Desktop/Di/edx/Analytic Edge/CPSData.csv")
table(CPS$Sex, is.na(CPS$Married))
tapply(is.na(CPS$MetroAreaCode), CPS$State, mean)
sort(tapply(CPS$Race == "Asian" ,CPS$MetroArea,  mean))
CPS = merge(CPS, MetroAreaMap, by.x="MetroAreaCode", by.y="Code", all.x=TRUE)
MetroAreaMap=read.csv("C:/Users/WALLI/Desktop/Di/edx/Analytic Edge/MetroAreaCodes.csv")
CPS = merge(CPS, MetroAreaMap, by.x="MetroAreaCode", by.y="Code", all.x=TRUE)
sort(tapply(CPS$Country == "India", CPS$MetroArea, sum, na.rm=TRUE))
wine= read.csv("C:/Users/WALLI/Desktop/Di/edx/Analytic Edge/wine.csv")
model = lm(Price ~ AGST, data = wine )
summary(model)
SSE=sum(model$residuals^2)
SSE
cor(wine$WinterRain, wine$Price)
cor(wine)
wineTest=read.csv("./edx/Analytic Edge/wine_test.csv")
wineTest=read.csv("C:/Users/WALLI/Desktop/Di/edx/Analytic Edge//wine_test.csv")
predictTest=predict(model, newdata = wineTest)
predictTest
SSE=sum((wineTest$Price - predictTest)^2)
SST=sum((wineTest$Price - mean(wine$Price))^2)
1-SSE/SST
pisa_train = read.csv("C:/Users/WALLI/Desktop/Di/edx/Analytic Edge/pisa2009train.csv")
pisa_test= read.csv("C:/Users/WALLI/Desktop/Di/edx/Analytic Edge/pisa2009test.csv")
pisa_train$raceeth = relevel(pisa_train$raceeth, "White")
pisa_test$raceeth = relevel(pisa_test$raceeth, "White")
pisa_test_pr=predict(lm_pisa, newdata = pisa_test)
library(caTools)
quality = read.csv("quality.csv")
library(caTools)
quality = read.csv("C:/Users/WALLI/Desktop/Di/edx/Analytic Edge/quality.csv")
# Randomly split data
set.seed(88)
split = sample.split(quality$PoorCare, SplitRatio = 0.75)
split
# Create training and testing sets
qualityTrain = subset(quality, split == TRUE)
qualityTest = subset(quality, split == FALSE)
QualityLog = glm(PoorCare ~ OfficeVisits + Narcotics, data=qualityTrain, family=binomial)
summary(QualityLog)
predictTrain = predict(QualityLog, type="response")
summary(predictTrain)
tapply(predictTrain, qualityTrain$PoorCare, mean)
table(qualityTrain$PoorCare, predictTrain > 0.5)
library(ROCR)
ROCRpred = prediction(predictTrain, qualityTrain$PoorCare)
# Performance function
ROCRperf = performance(ROCRpred, "tpr", "fpr")
# Plot ROC curve
plot(ROCRperf)
# Add colors
plot(ROCRperf, colorize=TRUE)
# Add threshold labels
plot(ROCRperf, colorize=TRUE, print.cutoffs.at=seq(0,1,by=0.1), text.adj=c(-0.2,1.7))
predictTest = predict(QualityLog, type="response", newdata=qualityTest)
ROCRpredTest = prediction(predictTest, qualityTest$PoorCare)
auc = as.numeric(performance(ROCRpredTest, "auc")@y.values)
# Multiple imputation
polling = read.csv("PollingData.csv")
# Multiple imputation
polling = read.csv("C:/Users/WALLI/Desktop/Di/edx/Analytic Edge/PollingData.csv")
# Multiple imputation
polling = read.csv("C:/Users/WALLI/Desktop/Di/edx/Analytic Edge/PollingData.csv")
simple = polling[c("Rasmussen", "SurveyUSA", "PropR", "DiffCount")]
summary(simple)
set.seed(144)
imputed = complete(mice(simple))
# Multiple imputation
polling = read.csv("C:/Users/WALLI/Desktop/Di/edx/Analytic Edge/PollingData.csv")
simple = polling[c("Rasmussen", "SurveyUSA", "PropR", "DiffCount")]
summary(simple)
set.seed(144)
library("mice")
install.packages("mice")
# Multiple imputation
polling = read.csv("C:/Users/WALLI/Desktop/Di/edx/Analytic Edge/PollingData.csv")
simple = polling[c("Rasmussen", "SurveyUSA", "PropR", "DiffCount")]
summary(simple)
set.seed(144)
library("mice")
imputed = complete(mice(simple))
summary(imputed)
polling$Rasmussen = imputed$Rasmussen
polling$SurveyUSA = imputed$SurveyUSA
summary(polling)
songtest=subset(songs, songs$year > 2009)
# To do this, we can use the following trick. First define a vector of variable names called nonvars #- these are the variables that we won't use in our model.
songs=read.csv("C:/Users/WALLI/Desktop/Di/edx/Analytic Edge/songs.csv")
songtrain=subset(songs, songs$year <= 2009)
songtest=subset(songs, songs$year > 2009)
nonvars = c("year", "songtitle", "artistname", "songID", "artistID")
#To remove these variables from your training and testing sets, type the following commands in your R #console:
songtrain = songtrain[ , !(names(songtrain) %in% nonvars) ]
songtest = songtest[ , !(names(songtest) %in% nonvars) ]
SongsLog2 = glm(Top10 ~ . - loudness, data=songtrain, family=binomial)
library(ROCR)
ROCRpred = prediction(predictTrain, qualityTrain$PoorCare)
# Performance function
ROCRperf = performance(ROCRpred, "tpr", "fpr")
# Plot ROC curve
plot(ROCRperf)
# Add colors
plot(ROCRperf, colorize=TRUE)
# Add threshold labels
plot(ROCRperf, colorize=TRUE, print.cutoffs.at=seq(0,1,by=0.1), text.adj=c(-0.2,1.7))
str(test)
talbe(test$violator, test_predict > 0.5)
table(test$violator, test_predict > 0.5)
12/(12+11)
167/(167+12)
(167+12)/(length(test_predict))
179/(179+23)
library("ROCR")
rocp=prediction(test_predict, test$violator)
as.numeric(performance(rocp, "auc")@y.values)
loans=read.csv("loans.csv")
table(loans$not.fully.paid)
1533/(1533+8045)
summary(loans)
table(loans$not.fully.paid)
nas_loans=subset(loans, is.na(loans$pub.rec))
nas_loans=subset(loans, is.na(loans$pub.rec), is.na(loans$delinq.2yrs))
nas_loans=subset(loans, is.na(loans$pub.rec) | is.na(loans$delinq.2yrs))
nas_loans=subset(loans, is.na(loans$pub.rec) )
nas_loans=subset(loans, is.na(loans$pub.rec) | is.na(loans$revol.util))
nas_loans=subset(loans, is.na(loans$pub.rec) | is.na(loans$revol.util) | is.na(loans$delinq.2yrs) | is.na(loans$inq.last.6mths) | is.na(loans$days.with.cr.line) | is.na(loans$log.annual.inc))
nas_loans
str(nas_loans)
summary(nas_loans)
table(nas_loans$not.fully.paid)
Claims = read.csv("C:\\Users\\WALLI\\Desktop\\Di\\edx\\Analytic EdgeClaimsData.csv")
Claims = read.csv("C:\\Users\\WALLI\\Desktop\\Di\\edx\\Analytic Edge\\ClaimsData.csv")
Claims = read.csv("C:\\Users\\WALLI\\Desktop\\Di\\edx\\Analytic Edge\\ClaimsData.csv")
# Split the data
library(caTools)
set.seed(88)
spl = sample.split(Claims$bucket2009, SplitRatio = 0.6)
ClaimsTrain = subset(Claims, spl==TRUE)
ClaimsTest = subset(Claims, spl==FALSE)
summary(ClaimsTrain)
q-subset(ClaimsTrain, alzheimers==1 | arthritis)
q-subset(ClaimsTrain, alzheimers==1 | arthritis==1)
q-subset(ClaimsTrain, ClaimsTrain$alzheimers==1 | ClaimsTrain$arthritis==1)
q=subset(ClaimsTrain, ClaimsTrain$alzheimers==1 | ClaimsTrain$arthritis==1)
q
summary(ClaimsTrain)
q=subset(ClaimsTrain, ClaimsTrain$alzheimers==1 | ClaimsTrain$arthritis==1 | cancer==1)
table(q)
sumaary(q)
summary(q)
q=subset(ClaimsTrain, ClaimsTrain$alzheimers==1 | ClaimsTrain$arthritis==1 | cancer==1 | copd=1 | depression==1 | diabetes==1 | heart.failure==1 | ihd==1 |kidney==1|  osteoporosis==1 | stroke ==1)
q=subset(ClaimsTrain, ClaimsTrain$alzheimers==1 | ClaimsTrain$arthritis==1 | cancer==1 | copd==1 | depression==1 | diabetes==1 | heart.failure==1 | ihd==1 |kidney==1|  osteoporosis==1 | stroke ==1)
View(polling)
168264/274803
table(ClaimsTrain$diabetes)
104672/170131
104672/(104672+170131)
table(ClaimsTrain$diabetes)/nrow(ClaimsTrain)
table(ClaimsTest$bucket2009, ClaimsTest$bucket2008)
PenaltyMatrix = matrix(c(0,1,2,3,4,2,0,1,2,3,4,2,0,1,2,6,4,2,0,1,8,6,4,2,0), byrow=TRUE, nrow=5)
# Penalty Matrix
PenaltyMatrix = matrix(c(0,1,2,3,4,2,0,1,2,3,4,2,0,1,2,6,4,2,0,1,8,6,4,2,0), byrow=TRUE, nrow=5)
PenaltyMatrix
# Penalty Error of Baseline Method
as.matrix(table(ClaimsTest$bucket2009, ClaimsTest$bucket2008))*PenaltyMatrix
sum(as.matrix(table(ClaimsTest$bucket2009, ClaimsTest$bucket2008))*PenaltyMatrix)/nrow(ClaimsTest)
table(ClaimsTest$bucket2009, ClaimsTest$bucket2008)
(110138+7787+3427+1452+174)/nrow(ClaimsTest)
PenaltyMatrix = matrix(c(0,0,0,0,0,2,2,2,2,2,4,4,4,4,4,6,6,6,6,6,8,8,8,8,8), byrow=TRUE, nrow=5)
PenaltyMatrix
sum(as.matrix(table(ClaimsTest$bucket2009, ClaimsTest$bucket2008))*PenaltyMatrix)/nrow(ClaimsTest)
library(rpart)
library(rpart.plot)
# CART model
ClaimsTree = rpart(bucket2009 ~ age + alzheimers + arthritis + cancer + copd + depression + diabetes + heart.failure + ihd + kidney + osteoporosis + stroke + bucket2008 + reimbursement2008, data=ClaimsTrain, method="class", cp=0.00005)
prp(ClaimsTree)
# CART model
# New CART model with loss matrix
ClaimsTree = rpart(bucket2009 ~ age + alzheimers + arthritis + cancer + copd + depression + diabetes + heart.failure + ihd + kidney + osteoporosis + stroke + bucket2008 + reimbursement2008, data=ClaimsTrain, method="class", cp=0.00005, parms=list(loss=PenaltyMatrix))
PenaltyMatrix = matrix(c(0,1,2,3,4,2,0,1,2,3,4,2,0,1,2,6,4,2,0,1,8,6,4,2,0), byrow=TRUE, nrow=5)
# CART model
# New CART model with loss matrix
ClaimsTree = rpart(bucket2009 ~ age + alzheimers + arthritis + cancer + copd + depression + diabetes + heart.failure + ihd + kidney + osteoporosis + stroke + bucket2008 + reimbursement2008, data=ClaimsTrain, method="class", cp=0.00005, parms=list(loss=PenaltyMatrix))
#prp(ClaimsTree)
PredictTest = predict(ClaimsTree, newdata = ClaimsTest, type = "class")
table(ClaimsTest$bucket2009, PredictTest)
94310/length(predictTest)
94310/nrow(ClaimsTest)
boston = read.csv("C:\\Users\\WALLI\\Desktop\\Di\\edx\\Analytic Edge\\boston.csv")
# Plot observations
plot(boston$LON, boston$LAT)
# Tracts alongside the Charles River
points(boston$LON[boston$CHAS==1], boston$LAT[boston$CHAS==1], col="blue", pch=19)
# Plot MIT
points(boston$LON[boston$TRACT==3531],boston$LAT[boston$TRACT==3531],col="red", pch=20)
summary(boston$NOX)
points(boston$LON[boston$NOX>=0.55], boston$LAT[boston$NOX>=0.55], col="green", pch=20)
# Plot observations
plot(boston$LON, boston$LAT)
# Tracts alongside the Charles River
points(boston$LON[boston$CHAS==1], boston$LAT[boston$CHAS==1], col="blue", pch=19)
# Plot MIT
points(boston$LON[boston$TRACT==3531],boston$LAT[boston$TRACT==3531],col="red", pch=20)
# Plot polution
summary(boston$NOX)
points(boston$LON[boston$NOX>=0.55], boston$LAT[boston$NOX>=0.55], col="green", pch=20)
# Plot prices
plot(boston$LON, boston$LAT)
summary(boston$MEDV)
points(boston$LON[boston$MEDV>=21.2], boston$LAT[boston$MEDV>=21.2], col="red", pch=20)
(0:10)
(0:10)*0.001
gerber=read.csv("C:\\Users\\WALLI\\Desktop\\Di\\edx\\Analytic Edge\\gerber.csv")
table(gerber$voting)
summary(gerber)
tapply(gerber$voting, gerber$civicduty, mean)
tapply(gerber$voting, gerber$civicduty, mean, na.rm=TRUE)
tapply(gerber$voting, gerber$hawthorne, mean, na.rm=TRUE)
tapply(gerber$voting, gerber$neighbors, mean, na.rm=TRUE)
tapply(gerber$voting, gerber$self, mean, na.rm=TRUE)
lr1=glm(voting ~ civicduty + hawthorne + neighbors + self, data = gerber)
summary(lm1)
summary(lr1)
predict(lr1, type=response)
predict(lr1, type="response")
table(gerber$voting, predict(lr1, type="response") > 0.4)
table(gerber$voting, predict(lr1, type="response") > 0.3)
(134513+51966)/nrow(gerber)
table(gerber$voting, predict(lr1, type="response") > 0.5)
235388/nrow(gerber)
library(ROCR)
gerber_pred=predict(lr1, type="response")
rocr_gerber=prediction(gerber_pred, gerber$voting)
as.numeric(performance(rocr_gerber, "auc")@y.values)
CARTmodel = rpart(voting ~ civicduty + hawthorne + self + neighbors, data=gerber)
library(rpart)
CARTmodel = rpart(voting ~ civicduty + hawthorne + self + neighbors, data=gerber)
library(rpart.plot)
prp(CARTmodel)
CARTmodel2 = rpart(voting ~ civicduty + hawthorne + self + neighbors, data=gerber, cp=0.0)
prp(CARTmodel2)
CARTmodel3 = rpart(voting ~ civicduty + hawthorne + self + neighbors+sex, data=gerber, cp=0.0)
prp(CARTmodel3)
CARTmodel3 = rpart(voting ~ civicduty + hawthorne + self + neighbors + sex, data=gerber, cp=0.0)
prp(CARTmodel3)
summary(gerber)
CARTmodelcontrl = rpart(voting ~ control, data=gerber, cp=0.0)
CARTmodelcontrlse = rpart(voting ~ control+sex, data=gerber, cp=0.0)
prp(CARTmodelcontrl)
prp(CARTmodelcontrl, digits = 6)
0.296638-0.34
prp(CARTmodelcontrlse, digits = 6)
0.334176-0.290456
0.345818-0.302795
lrsex=glm(voting ~ control + sex , data = gerberl, family = binomial)
lrsex=glm(voting ~ control + sex , data = gerber, family = binomial)
summary(lrsex)
Possibilities = data.frame(sex=c(0,0,1,1),control=c(0,1,0,1))
Possibilities
predict(lrsex, newdata=Possibilities, type="response")
0.290456-0.2908065
LogModel2 = glm(voting ~ sex + control + sex:control, data=gerber, family="binomial")
summary(LogModel2)
predict(LogModel2, newdata=Possibilities, type="response")
0.290456-0.2904558
0.00002
letters
letters=read.csv("C:\\Users\\WALLI\\Desktop\\Di\\edx\\Analytic Edge\\letters_ABPR.csv ")
letters$isB = as.factor(letters$letter == "B")
library(caTools)
set.seed(1000)
spl=sample.split(letters$isB, SplitRatio = 0.5)
letters_train=subset(letters, spl == TRUE)
letters_test=subset(letters, spl != TRUE)
table(letters_test$isB)
1175/(1175+383)
CARTb = rpart(isB ~ . - letter, data=train, method="class")
CARTb = rpart(isB ~ . - letter, data=letters_train, method="class")
cartb_predict=predict(CARTb, newdata = letters_test)
cartb_predict
cartb_predict=predict(CARTb, newdata = letters_test,type = "class")
table(letters_test$isB, cartb_predict)
(1118+340)/nrow(letters_test)
library(randomForest)
set.seed(1000)
rforest=randomForest(isB ~ . - letter, data= letters_train)
rforest[1]
rforest[1,]
rforest_predict = predict(rforest, newdata = letters_test, type="class")
table(letters_test$isB, rforest_predict)
(1165+374)/nrow(letters_test)
letters$letter = as.factor( letters$letter )
set.seed(2000)
spl=sample.split(letters$letter, SplitRatio = 0.5)
letters_train=subset(letters, spl == TRUE)
letters_train=subset(letters, spl != TRUE)
table(letters_test$letter)
396/nrow(letters_test)
396/(385+383+396+394)
letters_test=subset(letters, spl != TRUE)
table(letters_test$letter)
401/nrow(letters_test)
cart4letter=rpart(letter ~ . -isB, data=letters_train, method = "class")
cart4letter_pred=predict(cart4letter, newdata = letters_test, type = "class")
table(letters_test$letter, cart4letter_pred)
(366+325+359+349)/nrow(letters_test)
letters_train=subset(letters, spl == TRUE)
cart4letter=rpart(letter ~ . -isB, data=letters_train, method = "class")
cart4letter_pred=predict(cart4letter, newdata = letters_test, type = "class")
table(letters_test$letter, cart4letter_pred)
(348+318+363+340)/nrow(letters_test)
set.seed(1000)
rforest4letter=randomForest(letter ~ . - isB, data=letters_train)
rforest4letter_pred=predict(rforest4letter, newdata=letters_test, type = "class")
table(letters_test$letter, rforest4letter_pred)
390+380+393+364
1527/nrow(letters_test)
install.packages(ROCR)
install.packages("ROCR")
install.packages("rmarkdown")
install.packages()
?install.packages()
install.packages(c("ROCR", "caTools"))
2*2
a=read.csv()
a=read.csv("E:\Di\ozu.csv")
a=read.csv("E:\\Di\\ozu.csv")
a
a=read.csv("E:\\Di\\ozu.csv", sep="\t")
a
names(a)
labels(a)
a[5,]
a[,5]
rowsum(a[,5])
as.numeric(1 588)
as.numeric("1 588")
as.numeric(factor("1 588"))
a=read.csv("E:\\Di\\ozu.csv", sep="\t")
rowsum(a[,5])
(a[,5])
str(a)
(a[,5])
class(a[,5])
rowsum(a[,5])
a[,5]
a[,5][1]
a[,5][2]
sum(a[,5])
sum(a[,5])/1024
ls()
ls()
census = read.csv("E:\Di\edx\Analytic Edge")
census = read.csv("E:\\Di\\edx\\Analytic Edge\\census.csv")
set.seed(2000)
library(caTools)
set.seed(2000)
spl=sample(over50k, 0.6)
spl=sample(census$over50k, 0.6)
census_train = subset(census, spl == T)
census_test = subset(census, spl == F)
set.seed(2000)
spl=sample.split(census$over50k, 0.6)
census_train = subset(census, spl == T)
census_test = subset(census, spl == F)
glm1=glm(over50k ~ ., data=census_train, family = binomial)
summary(glm1)
savehistory("E:/Di/R proj/practice/.Rhistory")
