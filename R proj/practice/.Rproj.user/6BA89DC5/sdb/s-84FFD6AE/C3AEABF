{
    "collab_server" : "",
    "contents" : "---\ntitle: \"R Notebook\"\noutput: html_notebook\n---\n\nThis is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. \n\nTry executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. \n\n```{r}\nplot(cars)\n```\n\nAdd a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.\n\nWhen you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).\n\nRun this command when you are working on any lectures or exercises that might depend on the English language (for example, the names for the days of the week).\n```{r}\nSys.setlocale(\"LC_ALL\", \"C\")\n```\n\n```{r}\nseq(0,100,2)\n\n```\n\nrbind() combine dataframes by stacking rows\n\n```{r}\ngetwd()\nWHO=read.csv(\"C:\\\\Users\\\\WALLI\\\\Desktop\\\\Di\\\\edx\\\\Analytic Edge\\\\WHO.csv\")\nstr(WHO)\n\n```\n```{r}\nsummary(WHO)\nwhich.min(WHO$Under15)\nWHO$Country[86]\nWHO$Country[which.min(WHO$Over60)]\n\n```\n\n```{r}\nWHO_Europe = subset(WHO, Region  ==\"Europe\")\nstr(WHO_Europe)\n```\n\n```{r}\nhist(WHO$CellularSubscribers)\n```\n\n```{r}\nboxplot(WHO$LifeExpectancy ~ WHO$Region, xlab = \" \",  ylab=\"life expectancy\", main = \"Life expectancy of countries by region\")\n```\nOutliers computed as any points greater than the third quartile plus 1.5*IQR, or less than the first quartile minus 1.5*IQR. \n```{r}\ntable(WHO$Region)\ntapply(WHO$Over60, WHO$Region, mean)\n```\n```{r}\ntapply(WHO$LiteracyRate, WHO$Region, min, na.rm = TRUE)\n```\n```{r}\nUSDA=read.csv(\"C:\\\\Users\\\\WALLI\\\\Desktop\\\\Di\\\\edx\\\\Analytic Edge\\\\USDA.csv\")\n```\n```{r}\nhist(USDA$VitaminC, xlab = \" Vitamin C(mg)\", xlim = c(0,100), breaks = 100)\n```\nbreaks = 100 apply for entire range `r max(USDA$VitaminC, na.rm = TRUE)` - `r min(USDA$VitaminC, na.rm = TRUE)` so we get 2400/100 =\n\nAdding new variable\n```{r}\nHighSodium = as.numeric(USDA$Sodium > mean(USDA$Sodium, na.rm= TRUE))\nstr(HighSodium)\nUSDA$HighSodium = HighSodium\n```\n\n![Using tapply()](./Rnotebook_files/tapply.png)\n\n```{r}\nUSDA$HighFat = as.numeric(USDA$TotalFat > mean(USDA$TotalFat, na.rm= TRUE))\ntable(USDA$HighSodium, USDA$HighFat)\ntapply(USDA$VitaminC,USDA$HighSodium, summary, na.rm = TRUE)\n```\n\nDate conversion\n```{r}\nmvt=read.csv(\"C:/Users/WALLI/Desktop/Di/edx/Analytic Edge/mvtWeek1.csv\")\nDateConvert = as.Date(strptime(mvt$Date, \"%m/%d/%y %H:%M\"))\nsummary(DateConvert)\n```\n\n```{r}\nmvt$Month = months(DateConvert)\n\nmvt$Weekday = weekdays(DateConvert)\nmvt$Date = DateConvert\n```\n```{r}\nsort(table(mvt$LocationDescription))\n```\n\n```{r}\nTop5=subset(mvt, mvt$LocationDescription %in% c(\"STREET\", \"PARKING LOT/GARAGE(NON.RESID.)\", \"ALLEY\", \"GAS STATION\", \"DRIVEWAY - RESIDENTIAL\" ))\nTop5$LocationDescription = factor(Top5$LocationDescription)\n```\n```{r}\nplot(CocaCola$Date, CocaCola$StockPrice, type = \"l\", col = \"red\")\nlines(ProcterGamble$Date, ProcterGamble$StockPrice, col=\"blue\") \nabline(v=as.Date(c(\"2000-03-01\")), lwd=2)\n```\n\n\n```{r}\ntapply(IBM$StockPrice, months(IBM$Date), summary)\n```\n\nAnalysing NA values\n```{r}\nCPS=read.csv(\"C:/Users/WALLI/Desktop/Di/edx/Analytic Edge/CPSData.csv\")\ntable(CPS$Sex, is.na(CPS$Married))\n```\n\nComputing proportions\n```{r}\ntapply(is.na(CPS$MetroAreaCode), CPS$State, mean)\n\n```\n\n\n```{r}\nsort(tapply(CPS$Race == \"Asian\" ,CPS$MetroArea,  mean))\n```\n\n\nMerging\n\n```{r}\nMetroAreaMap=read.csv(\"C:/Users/WALLI/Desktop/Di/edx/Analytic Edge/MetroAreaCodes.csv\")\nCPS = merge(CPS, MetroAreaMap, by.x=\"MetroAreaCode\", by.y=\"Code\", all.x=TRUE)\n\n```\n\nThe first two arguments determine the data frames to be merged (they are called \"x\" and \"y\", respectively, in the subsequent parameters to the merge function). by.x=\"MetroAreaCode\" means we're matching on the MetroAreaCode variable from the \"x\" data frame (CPS), while by.y=\"Code\" means we're matching on the Code variable from the \"y\" data frame (MetroAreaMap). Finally, all.x=TRUE means we want to keep all rows from the \"x\" data frame (CPS), even if some of the rows' MetroAreaCode doesn't match any codes in MetroAreaMap (for those familiar with database terminology, this parameter makes the operation a left outer join instead of an inner join).\n\n\n\n\nTo obtain the number of TRUE values in a vector of TRUE/FALSE values, you can use the sum() function. For instance, sum(c(TRUE, FALSE, TRUE, TRUE)) is 3. Therefore, we can obtain counts of people born in a particular country living in a particular metropolitan area with:\n\n```{r}\nsort(tapply(CPS$Country == \"India\", CPS$MetroArea, sum, na.rm=TRUE))\n```\n\n\n\nLinera regression \n\n```{r}\nwine= read.csv(\"C:/Users/WALLI/Desktop/Di/edx/Analytic Edge/wine.csv\")\nmodel = lm(Price ~ AGST, data = wine )\nsummary(model)\n```\n\nSSE\n```{r}\nSSE=sum(model$residuals^2)\nSSE\n```\n\nCorrelation \n\n```{r}\ncor(wine$WinterRain, wine$Price)\ncor(wine)\n```\n\nPrediction\n\n```{r}\nwineTest=read.csv(\"C:/Users/WALLI/Desktop/Di/edx/Analytic Edge//wine_test.csv\")\npredictTest=predict(model, newdata = wineTest)\npredictTest\nSSE=sum((wineTest$Price - predictTest)^2)\nSST=sum((wineTest$Price - mean(wine$Price))^2)\n1-SSE/SST\n```\n\nChange reference level for factor variable\n```{r}\npisa_train = read.csv(\"C:/Users/WALLI/Desktop/Di/edx/Analytic Edge/pisa2009train.csv\")\npisa_test= read.csv(\"C:/Users/WALLI/Desktop/Di/edx/Analytic Edge/pisa2009test.csv\")\npisa_train$raceeth = relevel(pisa_train$raceeth, \"White\")\n\npisa_test$raceeth = relevel(pisa_test$raceeth, \"White\")\n```\n\nRMSE\n```{r}\npisa_test_pr=predict(lm_pisa, newdata = pisa_test)\n```\n\nLibrary for spliting data into data and test\n```{r}\nlibrary(caTools)\nquality = read.csv(\"C:/Users/WALLI/Desktop/Di/edx/Analytic Edge/quality.csv\")\n# Randomly split data\nset.seed(88)\nsplit = sample.split(quality$PoorCare, SplitRatio = 0.75)\nsplit\n\n# Create training and testing sets\nqualityTrain = subset(quality, split == TRUE)\nqualityTest = subset(quality, split == FALSE)\n```\n\nLogistic Regression Model\n```{r}\nQualityLog = glm(PoorCare ~ OfficeVisits + Narcotics, data=qualityTrain, family=binomial)\nsummary(QualityLog)\npredictTrain = predict(QualityLog, type=\"response\")\n```\n\nAnalyze predictions\n```{r}\nsummary(predictTrain)\n\ntapply(predictTrain, qualityTrain$PoorCare, mean)\n```\n\n# Confusion matrix for threshold of 0.5\n\n```{r}\ntable(qualityTrain$PoorCare, predictTrain > 0.5)\n```\n\n\nGenerate ROC curve\n```{r}\nlibrary(ROCR)\nROCRpred = prediction(predictTrain, qualityTrain$PoorCare)\n# Performance function\nROCRperf = performance(ROCRpred, \"tpr\", \"fpr\")\n\n# Plot ROC curve\nplot(ROCRperf)\n\n# Add colors\nplot(ROCRperf, colorize=TRUE)\n\n# Add threshold labels \nplot(ROCRperf, colorize=TRUE, print.cutoffs.at=seq(0,1,by=0.1), text.adj=c(-0.2,1.7))\n```\n\n\nComputing AUC\nThe AUC of a model has the following nice interpretation: given a random patient from the dataset who actually received poor care, and a random patient from the dataset who actually received good care, the AUC is the perecentage of time that our model will classify which is which correctly.\n```{r}\npredictTest = predict(QualityLog, type=\"response\", newdata=qualityTest)\nROCRpredTest = prediction(predictTest, qualityTest$PoorCare)\n\nauc = as.numeric(performance(ROCRpredTest, \"auc\")@y.values)\n```\n\n\n```{r}\n\n# Multiple imputation\npolling = read.csv(\"C:/Users/WALLI/Desktop/Di/edx/Analytic Edge/PollingData.csv\")\nsimple = polling[c(\"Rasmussen\", \"SurveyUSA\", \"PropR\", \"DiffCount\")]\nsummary(simple)\nset.seed(144)\nlibrary(\"mice\")\nimputed = complete(mice(simple))\nsummary(imputed)\npolling$Rasmussen = imputed$Rasmussen\npolling$SurveyUSA = imputed$SurveyUSA\nsummary(polling)\n\n```\n\nRemove variables from datasets\n\n```{r}\n# To do this, we can use the following trick. First define a vector of variable names called nonvars #- these are the variables that we won't use in our model.\nsongs=read.csv(\"C:/Users/WALLI/Desktop/Di/edx/Analytic Edge/songs.csv\")\nsongtrain=subset(songs, songs$year <= 2009)\nsongtest=subset(songs, songs$year > 2009)\nnonvars = c(\"year\", \"songtitle\", \"artistname\", \"songID\", \"artistID\")\n\n#To remove these variables from your training and testing sets, type the following commands in your R #console:\n\nsongtrain = songtrain[ , !(names(songtrain) %in% nonvars) ]\n\nsongtest = songtest[ , !(names(songtest) %in% nonvars) ]\n```\n\nSubstract variable from model\n```{r}\nSongsLog2 = glm(Top10 ~ . - loudness, data=songtrain, family=binomial)\n```\nBut this approach (subtracting the variable from the model formula) will always work when you want to remove *numeric variables*.\n\n\n\nTrees\n```{r}\nstevens = read.csv(\"C:\\\\Users\\\\WALLI\\\\Desktop\\\\Di\\\\edx\\\\Analytic Edge\\\\stevens.csv\")\nlibrary(caTools)\nset.seed(3000)\nspl = sample.split(stevens$Reverse, SplitRatio = 0.7)\nTrain = subset(stevens, spl==TRUE)\nTest = subset(stevens, spl==FALSE)\nlibrary(rpart)\nlibrary(rpart.plot)\n\n# CART model\nStevensTree = rpart(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, method=\"class\", minbucket=25)\nprp(StevensTree)\n\n```\n\n```{r}\n# ROC curve\nlibrary(ROCR)\n\nPredictROC = predict(StevensTree, newdata = Test)\nPredictROC\n\npred = prediction(PredictROC[,2], Test$Reverse)\nperf = performance(pred, \"tpr\", \"fpr\")\nplot(perf)\n\n```\n\n\nRandom Forest\n```{r}\nlibrary(randomForest)\n# Build random forest model\nStevensForest = randomForest(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, ntree=200, nodesize=25 )\n#get warning\n\n# Convert outcome to factor\nTrain$Reverse = as.factor(Train$Reverse)\nTest$Reverse = as.factor(Test$Reverse)\n\n# Try again\nStevensForest = randomForest(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, ntree=200, nodesize=25 )\n\n```\nCross-validating\n```{r}\n# Define cross-validation experiment\nlibrary(caret)\n\nlibrary(e1071)\nnumFolds = trainControl( method = \"cv\", number = 10 )\ncpGrid = expand.grid( .cp = seq(0.01,0.5,0.01)) \n\n# Perform the cross validation\ntrain(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, method = \"rpart\", trControl = numFolds, tuneGrid = cpGrid )\n# Create a new CART model\n# Accuracy was used to select the optimal model using  the largest value.\n# The final value used for the model was cp = 0.18.\nStevensTreeCV = rpart(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, method=\"class\", cp = 0.18)\n\n```\n\nCART models\n```{r}\nClaims = read.csv(\"C:\\\\Users\\\\WALLI\\\\Desktop\\\\Di\\\\edx\\\\Analytic Edge\\\\ClaimsData.csv\")\n\n# Split the data\nlibrary(caTools)\n\nset.seed(88)\n\nspl = sample.split(Claims$bucket2009, SplitRatio = 0.6)\n\nClaimsTrain = subset(Claims, spl==TRUE)\n\nClaimsTest = subset(Claims, spl==FALSE)\n```\n\n```{r}\n# Penalty Matrix\nPenaltyMatrix = matrix(c(0,1,2,3,4,2,0,1,2,3,4,2,0,1,2,6,4,2,0,1,8,6,4,2,0), byrow=TRUE, nrow=5)\n\nPenaltyMatrix\n\n# Penalty Error of Baseline Method\nas.matrix(table(ClaimsTest$bucket2009, ClaimsTest$bucket2008))*PenaltyMatrix\n\nsum(as.matrix(table(ClaimsTest$bucket2009, ClaimsTest$bucket2008))*PenaltyMatrix)/nrow(ClaimsTest)\n\n```\n\n```{r}\n# CART model\n# New CART model with loss matrix\nClaimsTree = rpart(bucket2009 ~ age + alzheimers + arthritis + cancer + copd + depression + diabetes + heart.failure + ihd + kidney + osteoporosis + stroke + bucket2008 + reimbursement2008, data=ClaimsTrain, method=\"class\", cp=0.00005, parms=list(loss=PenaltyMatrix))\n\n\n#prp(ClaimsTree)\n\n```\n\n\n```{r}\n# Plot observations\nplot(boston$LON, boston$LAT)\n\n# Tracts alongside the Charles River\npoints(boston$LON[boston$CHAS==1], boston$LAT[boston$CHAS==1], col=\"blue\", pch=19)\n\n# Plot MIT\npoints(boston$LON[boston$TRACT==3531],boston$LAT[boston$TRACT==3531],col=\"red\", pch=20)\n# Plot polution\nsummary(boston$NOX)\npoints(boston$LON[boston$NOX>=0.55], boston$LAT[boston$NOX>=0.55], col=\"green\", pch=20)\n\n# Plot prices\nplot(boston$LON, boston$LAT)\nsummary(boston$MEDV)\npoints(boston$LON[boston$MEDV>=21.2], boston$LAT[boston$MEDV>=21.2], col=\"red\", pch=20)\n```\n\n```{r}\n\n\n```\n\nCP parameter:\n\n![Using CP](./Rnotebook_files/cp1.png)\n\n![\n\n](./Rnotebook_files/cp2.png)\n\nAs we discussed in lecture, random forest models work by building a large collection of trees. As a result, we lose some of the interpretability that comes with CART in terms of seeing how predictions are made and which variables are important. However, we can still compute metrics that give us insight into which variables are important.\n\nOne metric that we can look at is the number of times, aggregated over all of the trees in the random forest model, that a certain variable is selected for a split. To view this metric, run the following lines of R code (replace \"MODEL\" with the name of your random forest model):\n\nvu = varUsed(MODEL, count=TRUE)\n\nvusorted = sort(vu, decreasing = FALSE, index.return = TRUE)\n\ndotchart(vusorted$x, names(MODEL$forest$xlevels[vusorted$ix]))\n\nA different metric we can look at is related to \"impurity\", which measures how homogenous each bucket or leaf of the tree is. In each tree in the forest, whenever we select a variable and perform a split, the impurity is decreased. Therefore, one way to measure the importance of a variable is to average the reduction in impurity, taken over all the times that variable is selected for splitting in all of the trees in the forest. To compute this metric, run the following command in R (replace \"MODEL\" with the name of your random forest model):\n\nvarImpPlot(MODEL)\n\n\n\n\nText Analytics\n\n```{r}\ntweets = read.csv(\"C:\\\\Users\\\\DBelyakov\\\\education\\\\Analytic Edge\\\\tweets.csv\", stringsAsFactors=FALSE)\ntweets$Negative = as.factor(tweets$Avg <= -1)\nlibrary(tm)\nlibrary(SnowballC)\n\n# Create corpus\ncorpus = VCorpus(VectorSource(tweets$Tweet)) \n\n# Look at corpus\ncorpus\ncorpus[[1]]$content\n\n\n# Convert to lower-case\n\ncorpus = tm_map(corpus, content_transformer(tolower))\n\ncorpus[[1]]$content\n\n# Remove punctuation\n\ncorpus = tm_map(corpus, removePunctuation)\n\ncorpus[[1]]$content\n\n# Look at stop words \nstopwords(\"english\")[1:10]\n\n# Remove stopwords and apple\n\ncorpus = tm_map(corpus, removeWords, c(\"apple\", stopwords(\"english\")))\n\ncorpus[[1]]$content\n\n# Stem document \n\ncorpus = tm_map(corpus, stemDocument)\n\ncorpus[[1]]$content\n```\n\n```{r}\nfrequencies = DocumentTermMatrix(corpus)\ninspect(frequencies[1000:1005,505:515])\nfindFreqTerms(frequencies, lowfreq=20)\n```\n# Remove sparse terms\n```{r}\nsparse = removeSparseTerms(frequencies, 0.995)\nsparse\n```\n\n```{r}\n# Convert to a data frame\n\ntweetsSparse = as.data.frame(as.matrix(sparse))\n\n# Make all variable names R-friendly\n\ncolnames(tweetsSparse) = make.names(colnames(tweetsSparse))\n\n# Add dependent variable\n\ntweetsSparse$Negative = tweets$Negative\n```\n\n\n123\n",
    "created" : 1499763539510.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "32905177",
    "id" : "C3AEABF",
    "lastKnownWriteTime" : 1500060482,
    "last_content_update" : 1500060482996,
    "path" : "C:/Users/DBelyakov/education/R proj/practice/Rnotebook_sandbox.Rmd",
    "project_path" : "Rnotebook_sandbox.Rmd",
    "properties" : {
        "chunk_output_type" : "inline"
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}